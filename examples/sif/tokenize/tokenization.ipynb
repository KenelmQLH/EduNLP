{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization\n",
    "\n",
    "## 概述\n",
    "\n",
    "为了方便后续向量化表征试题，本模块提供题目文本的令牌化解析（Tokenization），即将题目转换成令牌序列。    \n",
    "\n",
    "根据构成题目的元素类型，解析功能分为 **“文本解析”** 和 **“公式解析”** 两部分。\n",
    "\n",
    "### 文本解析\n",
    "\n",
    "根据题目文本切分粒度的大小，文本解析又分为 **“句解析”** 和 **“词解析”**。\n",
    "\n",
    "（1） 句解析（sentence-tokenization）：将较长的文档切分成若干句子的过程称为“分句”。每个句子为一个“令牌”（token）。（待实现）    \n",
    "  \n",
    "\n",
    "（2） 词解析（text-tokenization）：一个句子（不含公式）是由若干“词”按顺序构成的，将一个句子切分为若干词的过程称为“词解析”。根据词的粒度大小，又可细分为“词组解析”和\"单字解析\"。\n",
    "- 词组解析 (word-tokenization)：每一个词组为一个“令牌”（token）。\n",
    "- 单字解析 (char-tokenization)：单个字符即为一个“令牌”（token）。\n",
    "\n",
    "### 公式解析\n",
    "\n",
    "公式解析（formula-tokenization）：理科类文本中常常含有公式。将一个符合 latex 语法的公式切分为标记字符列表的过程称为“公式解析”。每个标记字符为一个“令牌”（token）。  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 文本解析"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 句解析\n",
    "\n",
    "待实现..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 词解析\n",
    "\n",
    "词解析分为两个主要步骤：  \n",
    "\n",
    "（1） 分词：  \n",
    "- 词组解析：使用分词工具切分并提取题目文本中的词。  \n",
    "    本项目目前支持的分词工具有：`jieba`   \n",
    "- 单字解析：按字符划分。\n",
    "    \n",
    "      \n",
    "（2） 筛选：过滤指定的停用词。   \n",
    "- 本项目默认使用的停用词表：[stopwords](https://github.com/bigdata-ustc/EduNLP/blob/master/EduNLP/meta_data/sif_stopwords.txt)  \n",
    "- 你也可以使用自己的停用词表，具体使用方法见下面的示例。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入模块\n",
    "from EduNLP.SIF.tokenization.text import tokenize "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 输入\n",
    "text = \"三角函数是基本初等函数之一\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 词组解析\n",
    "\n",
    "分词粒度参数选择 word： `granularity = \"word\"` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['三角函数', '初等', '函数']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 输出：默认使用 EduNLP 项目提供的停用词表\n",
    "tokenize(text, granularity=\"word\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 单字解析\n",
    "\n",
    "分词粒度参数选择 word： `granularity = \"char\"` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['三', '角', '函', '数', '基', '初', '函', '数']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 输出：默认使用 EduNLP 项目提供的停用词表\n",
    "tokenize(text, granularity=\"char\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 停用词表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'一旦', '一时', '一来', '一样', '一次', '一片', '一番', '一直', '一致'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 获取自己的停用词表\n",
    "spath = \"test_stopwords.txt\"\n",
    "from EduNLP.SIF.tokenization.text.stopwords import get_stopwords\n",
    "stopwords = get_stopwords(spath)\n",
    "stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['三角函数', '是', '基本', '初等', '函数', '之一']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 输出：传入停用词表（stopwords）\n",
    "tokenize(text, granularity=\"word\", stopwords=stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 公式解析\n",
    "切分出 latex 公式的每个标记符号。针对本模块更加详细的解释参见 [formula](../formula/formula.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入模块\n",
    "from EduNLP.SIF.tokenization.formula import tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 输入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "formula = \"\\\\frac{\\\\pi}{x + y} + 1 = x\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 输出"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "（1）如果您想按 latex 语法标记拆分公式的各个部分，并得到顺序序列结果，输出方法可以选择：`linear`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\\\frac', '{', '\\\\pi', '}', '{', 'x', '+', 'y', '}', '+', '1', '=', 'x']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize(formula, method=\"linear\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "（2） 如果您想得到公式解析出的语法分析树序列，输出方法可以选择：`ast`\n",
    "> 抽象语法分析树，简称语法树（Syntax tree），是源代码语法结构的一种抽象表示。它以树状的形式表现编程语言的语法结构，树上的每个节点都表示源代码中的一种结构。  \n",
    "> 因此，ast 可以看做是公式的语法结构表征。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\\\pi', '{ }', 'x', '+', 'y', '{ }', '\\\\frac', '+', '1', '=', 'x']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize(formula, method=\"ast\", return_type=\"list\", ord2token=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "（3）如果您只是关心公式的结构和类型，并不关心变量具体是什么，比如二元二次方程 `x^2 + y = 1` ，它从公式结构和类型上来说，和 `w^2 + z = 1` 没有区别。  \n",
    "此时，您可以设置如下参数：`ord2token = True`，将公式变量名转换成 token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['mathord',\n",
       " '{ }',\n",
       " 'mathord',\n",
       " '+',\n",
       " 'mathord',\n",
       " '{ }',\n",
       " '\\\\frac',\n",
       " '+',\n",
       " 'textord',\n",
       " '=',\n",
       " 'mathord']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 输出形式选择抽象语法分析树（ast）且将公式变量名转换成 token\n",
    "tokenize(formula, method=\"ast\", return_type=\"list\", ord2token=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "（4） 如果您除了 （3） 中提供的功能之外，还需要区分不同的变量。此时可以另外设置参数：`var_numbering=True`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['mathord_con',\n",
       " '{ }',\n",
       " 'mathord_0',\n",
       " '+',\n",
       " 'mathord_1',\n",
       " '{ }',\n",
       " '\\\\frac',\n",
       " '+',\n",
       " 'textord',\n",
       " '=',\n",
       " 'mathord_0']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 输出形式选择抽象语法分析树（ast）且将公式变量名转换成带编号的 token\n",
    "tokenize(formula, method=\"ast\", return_type=\"list\", ord2token=True, var_numbering=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 综合解析\n",
    "\n",
    "综合解析，即综合以上两种解析方式（标记解析 + 公式解析），提供对题目文本的全解析。另外，如遇到特殊符号将转换成常量，例如：\n",
    "```python\n",
    "FIGURE_SYMBOL = \"[FIGURE]\" # $\\SIFChoice$\n",
    "QUES_MARK_SYMBOL = \"[MARK]\" # $\\FigureID{1}$\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 令牌化容器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\MySoftwares\\Anaconda\\envs\\data\\lib\\site-packages\\gensim\\similarities\\__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['如图',\n",
       " '古希腊',\n",
       " '数学家',\n",
       " '希波',\n",
       " '克拉底',\n",
       " '研究',\n",
       " '几何图形',\n",
       " '此图',\n",
       " '三个',\n",
       " '半圆',\n",
       " '三个',\n",
       " '半圆',\n",
       " '直径',\n",
       " '直角三角形',\n",
       " 'ABC',\n",
       " '斜边',\n",
       " 'BC',\n",
       " '直角',\n",
       " 'AB',\n",
       " 'AC',\n",
       " '\\x08',\n",
       " 'igtriangleupABC',\n",
       " '三边',\n",
       " '围成',\n",
       " '区域',\n",
       " '记',\n",
       " 'I',\n",
       " '黑色',\n",
       " '记',\n",
       " 'II',\n",
       " '其余部分',\n",
       " '记',\n",
       " 'III',\n",
       " '图形',\n",
       " '中',\n",
       " '随机',\n",
       " '取',\n",
       " '一点',\n",
       " '此点',\n",
       " '取自',\n",
       " 'I',\n",
       " ',',\n",
       " 'II',\n",
       " ',',\n",
       " 'III',\n",
       " '概率',\n",
       " '记',\n",
       " 'p',\n",
       " '_',\n",
       " '1',\n",
       " ',',\n",
       " 'p',\n",
       " '_',\n",
       " '2',\n",
       " ',',\n",
       " 'p',\n",
       " '_',\n",
       " '3',\n",
       " '[MARK]',\n",
       " '[FIGURE]']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 导入模块\n",
    "from EduNLP.Tokenizer import get_tokenizer\n",
    "\n",
    "# 输入\n",
    "item = [\n",
    "    \"如图来自古希腊数学家希波克拉底所研究的几何图形．此图由三个半圆构成，三个半圆的直径分别为直角三角形$ABC$的斜边$BC$, 直角边$AB$, $AC$.$\\bigtriangleup ABC$的三边所围成的区域记为$I$,黑色部分记为$II$, 其余部分记为$III$.在整个图形中随机取一点，此点取自$I,II,III$的概率分别记为$p_1,p_2,p_3$,则$\\SIFChoice$$\\FigureID{1}$\"\n",
    "]\n",
    "\n",
    "# 输出\n",
    "tokenizer = get_tokenizer(\"text\")\n",
    "tokens = tokenizer(item)\n",
    "next(tokens) "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
