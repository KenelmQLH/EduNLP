{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 使用QuesNet向量化容器\n",
    "## 导入功能块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from EduNLP.Tokenizer import DisenQTokenizer\n",
    "from EduNLP.Vector import T2V, DisenQModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = \"../..\"\n",
    "\n",
    "data_dir = f\"{BASE_DIR}/static/test_data\"\n",
    "output_dir = f\"{BASE_DIR}/examples/test_model/disenq\"\n",
    "\n",
    "# 对题目文本进行令牌化\n",
    "items = [\n",
    "    \"有 公 式 $\\\\FormFigureID{wrong1?}$ ，如 图 $\\\\FigureID{088f15ea-xxx}$\",\n",
    "    \"已知 圆 $x^{2}+y^{2}-6 x=0$ ，过 点 (1,2) 的 直 线 被 该 圆 所 截 得 的 弦 的 长度 的 最小 值 为\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 令牌化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = DisenQTokenizer.from_pretrained(output_dir)\n",
    "\n",
    "# 可以对单个题目进行令牌化\n",
    "print(tokenizer(items[0]))\n",
    "print()\n",
    "\n",
    "# 也可以对题目列表进行令牌化\n",
    "token_items = tokenizer(items)\n",
    "print(token_items)\n",
    "print()\n",
    "\n",
    "token_items = tokenizer(items)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 向量化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_dir = f\"{BASE_DIR}/examples/test_model/disenq\"\n",
    "t2v = DisenQModel(pretrained_dir)\n",
    "\n",
    "# 获得句表征和词表征\n",
    "t_vec, i_vec_k, i_vec_i = t2v(token_items)\n",
    "print(i_vec_k.shape, i_vec_i.shape)\n",
    "print(t_vec.shape)\n",
    "print()\n",
    "\n",
    "# 获得指定表征\n",
    "i_vec_k, i_vec_i = t2v.infer_vector(token_items, vector_type=\"k\")\n",
    "t_vec = t2v.infer_tokens(token_items)\n",
    "\n",
    "i_vec_k = t2v.infer_vector(token_items, vector_type=\"k\")\n",
    "i_vec_i = t2v.infer_vector(token_items, vector_type=\"i\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2a09bcfc86f5d80d5adfb774779878f28f4d48d5a6d6c0020bcfd8afaf909ec6"
  },
  "kernelspec": {
   "display_name": "Python 3.6.13 ('data')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.6.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
