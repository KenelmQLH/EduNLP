{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 使用QuesNet向量化容器\n",
    "## 导入功能块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\MySoftwares\\Anaconda\\envs\\data\\lib\\site-packages\\gensim\\similarities\\__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "from EduNLP.Pretrain import ElmoTokenizer\n",
    "from EduNLP.Vector import T2V, ElmoModel\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设置你的数据路径和输出路径\n",
    "BASE_DIR = \"../..\"\n",
    "\n",
    "data_dir = f\"{BASE_DIR}/static/test_data\"\n",
    "output_dir = f\"{BASE_DIR}/examples/test_model/elmo\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 令牌化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([527, 231, 3, 13, 26, 79, 159, 527, 6, 33, 10, 13, 34, 133, 79, 168, 4], 17)\n",
      "\n",
      "([[527, 231, 3, 13, 26, 79, 159, 527, 6, 33, 10, 13, 34, 133, 79, 168, 4], [7, 104, 13, 15, 16, 17, 18, 34, 79, 15, 16, 17, 18, 19, 105, 13, 10, 23, 106, 107, 104, 108, 109, 110, 111]], [17, 25])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 加载之前训练的模型tokenizer\n",
    "tokenizer = ElmoTokenizer(os.path.join(output_dir, \"vocab.json\"))\n",
    "\n",
    "# 对题目文本进行令牌化\n",
    "items = [\n",
    "    \"有公式$\\\\FormFigureID{wrong1?}$，如图$\\\\FigureID{088f15ea-xxx}$,\\\n",
    "    若$x,y$满足约束条件公式$\\\\FormFigureBase64{wrong2?}$,$\\\\SIFSep$，则$z=x+7 y$的最大值为$\\\\SIFBlank$\",\n",
    "    \"已知圆$x^{2}+y^{2}-6 x=0$，过点(1,2)的直线被该圆所截得的弦的长度的最小值为\"\n",
    "]\n",
    "\n",
    "# 可以对单个题目进行令牌化\n",
    "print(tokenizer(items[0], freeze_vocab=True))\n",
    "print()\n",
    "\n",
    "# 也可以对题目列表进行令牌化\n",
    "print(tokenizer(items, freeze_vocab=True))\n",
    "print()\n",
    "\n",
    "token_items, lengths = tokenizer(items, pad_to_max_length=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 向量化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 512])\n",
      "\n",
      "torch.Size([2, 512])\n",
      "torch.Size([2, 25, 512])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "t2v = ElmoModel(output_dir)\n",
    "\n",
    "# # 获得句表征\n",
    "i_vec = t2v(token_items)\n",
    "print(i_vec.shape)\n",
    "print()\n",
    "\n",
    "# 获得句表征和词表征\n",
    "i_vec = t2v.infer_vector(token_items, lengths=lengths)\n",
    "t_vec = t2v.infer_tokens(token_items, lengths=lengths)\n",
    "print(i_vec.shape)\n",
    "print(t_vec.shape)\n",
    "print()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2a09bcfc86f5d80d5adfb774779878f28f4d48d5a6d6c0020bcfd8afaf909ec6"
  },
  "kernelspec": {
   "display_name": "Python 3.6.13 ('data')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
