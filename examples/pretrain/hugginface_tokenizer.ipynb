{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/qlh/anaconda3/envs/py36/lib/python3.6/site-packages/gensim/similarities/__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "from EduNLP.Pretrain import TokenizerForHuggingface"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Huggingface通用化接口"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = TokenizerForHuggingface(\"bert-base-chinese\", add_specials=True, tokenize_method=\"ast_formula\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 基础用法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dump cache file failed.\n",
      "Traceback (most recent call last):\n",
      "  File \"/data/qlh/anaconda3/envs/py36/lib/python3.6/site-packages/jieba/__init__.py\", line 154, in initialize\n",
      "    _replace_file(fpath, cache_file)\n",
      "PermissionError: [Errno 1] Operation not permitted: '/tmp/tmpgu196tfc' -> '/tmp/jieba.cache'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['公', '式', '[FORMULA]', '公', '式', '[FORMULA]', '如', '图', '[FIGURE]', 'ma', '##th', '##or', '##d', '_', '0', ',', 'ma', '##th', '##or', '##d', '_', '1', '约', '束', '条', '件', '[SEP]', 'ma', '##th', '##or', '##d', '_', '2', '=', 'ma', '##th', '##or', '##d', '_', '0', '+', 'text', '##or', '##d', 'ma', '##th', '##or', '##d', '_', '1', '最', '大', '值', '[MARK]']\n",
      "{'input_ids': tensor([[  101,  1062,  2466, 21129,  1062,  2466, 21129,  1963,  1745, 21130,\n",
      "          9622,  8414,  8372,  8168,   142,   121,   117,  9622,  8414,  8372,\n",
      "          8168,   142,   122,  5276,  3338,  3340,   816,   102,  9622,  8414,\n",
      "          8372,  8168,   142,   123,   134,  9622,  8414,  8372,  8168,   142,\n",
      "           121,   116, 10539,  8372,  8168,  9622,  8414,  8372,  8168,   142,\n",
      "           122,  3297,  1920,   966, 21131,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "text = '有公式$\\\\FormFigureID{wrong1?}$和公式$\\\\FormFigureBase64{wrong2?}$，\\\n",
    "                    如图$\\\\FigureID{088f15ea-8b7c-11eb-897e-b46bfc50aa29}$,\\\n",
    "                    若$x,y$满足约束条件$\\\\SIFSep$，则$z=x+7 y$的最大值为$\\\\SIFBlank$'\n",
    "\n",
    "tokens = tokenizer.tokenize(text)\n",
    "print(tokens)\n",
    "\n",
    "\n",
    "encodes = tokenizer(text)\n",
    "print(encodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 分词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['公', '式', '[FORMULA]', '公', '式', '[FORMULA]', '如', '图', '[FIGURE]', 'ma', '##th', '##or', '##d', '_', '0', ',', 'ma', '##th', '##or', '##d', '_', '1', '约', '束', '条', '件', '[SEP]', 'ma', '##th', '##or', '##d', '_', '2', '=', 'ma', '##th', '##or', '##d', '_', '0', '+', 'text', '##or', '##d', 'ma', '##th', '##or', '##d', '_', '1', '最', '大', '值', '[MARK]']\n",
      "\n",
      "公式 [FORMULA] 公式 [FORMULA] 如图 [FIGURE] mathord_0 , mathord_1 约束条件 [SEP] mathord_2 = mathord_0 + textord mathord_1 最大值 [MARK]\n",
      "['公', '式', '[FORMULA]', '公', '式', '[FORMULA]', '如', '图', '[FIGURE]', 'ma', '##th', '##or', '##d', '_', '0', ',', 'ma', '##th', '##or', '##d', '_', '1', '约', '束', '条', '件', '[SEP]', 'ma', '##th', '##or', '##d', '_', '2', '=', 'ma', '##th', '##or', '##d', '_', '0', '+', 'text', '##or', '##d', 'ma', '##th', '##or', '##d', '_', '1', '最', '大', '值', '[MARK]']\n"
     ]
    }
   ],
   "source": [
    "tokens = tokenizer.tokenize(text)\n",
    "print(tokens)\n",
    "print()\n",
    "\n",
    "# 等价于如下操作\n",
    "pre_tokens = tokenizer._pre_tokenize(text)\n",
    "print(pre_tokens)\n",
    "tokens = tokenizer.bert_tokenizer.tokenize(pre_tokens)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 21130, 102]\n",
      "[CLS] [FIGURE] [SEP]\n"
     ]
    }
   ],
   "source": [
    "encode_idxs = tokenizer.encode(\"[FIGURE]\")\n",
    "print(encode_idxs)\n",
    "\n",
    "encode_tokens = tokenizer.decode(encode_idxs)\n",
    "print(encode_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 扩充词表"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 直接新增单词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[python]', 'is', 'a', 'co', '##ding', 'language']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.add_tokens([\"[python]\"])\n",
    "tokenizer.bert_tokenizer.tokenize(\"[python] is a coding language\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'[FIGURE]', '[FORMULA_END]', '[FORMULA]', '[SEP]', '[TEXT_BEGIN]', '[TEXT_END]', '[TEXT]', '[', 'n', '[TAG]', 'e', ']', 'w', '[FORMULA_BEGIN]', '[MARK]'}\n",
      "\n",
      "{'[FIGURE]', '[FORMULA_END]', '[FORMULA]', '[new]', '[SEP]', '[TEXT_BEGIN]', '[TEXT_END]', '[TEXT]', '[', 'n', '[TAG]', 'e', ']', 'w', '[FORMULA_BEGIN]', '[MARK]'}\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer._special_tokens)\n",
    "print()\n",
    "\n",
    "tokenizer.add_specials([\"[new]\"])\n",
    "print(tokenizer._special_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 批量设置语料库词表\n",
    "\n",
    "1. 根据原始文本更新词表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['公式', '[FORMULA]', '如图', '[FIGURE]', 'mathord_0', ',', 'mathord_1', '约束条件', '[SEP]', 'mathord_2', '=', '+', 'textord', '最大值', '[MARK]']\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "vocab_sentences = [\n",
    "   '有公式$\\\\FormFigureID{wrong1?}$和公式$\\\\FormFigureBase64{wrong2?}$，如图$\\\\FigureID{088f15ea-8b7c-11eb-897e-b46bfc50aa29}$, 若$x,y$满足约束条件$\\\\SIFSep$，则$z=x+7 y$的最大值为$\\\\SIFBlank$'\n",
    "]\n",
    "\n",
    "remain_tokens, added_num = tokenizer.set_vocab(vocab_sentences, lower=False, trim_min_count=1, do_tokenize=True)\n",
    "\n",
    "print(remain_tokens)\n",
    "print(added_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 根据分词序列更新词表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['公式', '[FORMULA]', '如图', '[FIGURE]', 'mathord_0', ',', 'mathord_1', '约束条件', '[SEP]', 'mathord_2', '=', '+', 'textord', '最大值', '[MARK]']\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "vocab_tokens = [\n",
    "   ['公式', '[FORMULA]', '公式', '[FORMULA]', '如图', '[FIGURE]', 'mathord_0', ',', 'mathord_1', '约束条件', '[SEP]', 'mathord_2', '=', 'mathord_0', '+', 'textord', 'mathord_1', '最大值', '[MARK]']\n",
    "]\n",
    "\n",
    "remain_tokens, added_num = tokenizer.set_vocab(vocab_tokens, lower=False, trim_min_count=1, do_tokenize=False)\n",
    "\n",
    "print(remain_tokens)\n",
    "print(added_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101, 21139,   102]]), 'token_type_ids': tensor([[0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1]])}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(\"公式\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 101, 1062, 2466,  102]]), 'token_type_ids': tensor([[0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(\"公 式\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 保存与加载"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存\n",
    "save_dir = \"./tmp\"\n",
    "tokenizer.save_pretrained(save_dir)\n",
    "\n",
    "# 加载\n",
    "tokenizer = TokenizerForHuggingface.from_pretrained(save_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.13 ('py36')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "672c49ef5d0c797ca83477c465883c954b68a3ad2765b748855bc549ed895b7b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
