{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\MySoftwares\\Anaconda\\envs\\data\\lib\\site-packages\\gensim\\similarities\\__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "from EduNLP.Pretrain import DisenQTokenizer, train_disenQNet\n",
    "from EduNLP.Vector import DisenQModel, T2V\n",
    "from EduNLP.I2V import DisenQ\n",
    "from EduNLP.ModelZoo import load_items\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]= \"0,1\"\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 训练自己的disenQNet模型\n",
    "## 1. 数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = \"E:\\Workustc\\EduNLP\\workMaster\\EduNLP\"\n",
    "\n",
    "data_dir = f\"{BASE_DIR}/tests/test_vec/test_data\"\n",
    "output_dir = f\"{BASE_DIR}/examples/test_model/data\"\n",
    "\n",
    "disen_data_train = load_items(f\"{data_dir}/disenq_train.json\")\n",
    "disen_data_test = load_items(f\"{data_dir}/disenq_test.json\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 训练和评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save vocab to E:\\Workustc\\EduNLP\\workMaster\\EduNLP/examples/test_model/data\\vocab.list\n",
      "save concept to E:\\Workustc\\EduNLP\\workMaster\\EduNLP/examples/test_model/data\\concept.list\n",
      "save word2vec to E:\\Workustc\\EduNLP\\workMaster\\EduNLP/examples/test_model/data\\wv.th\n",
      "processing raw data for QuestionDataset...\n",
      "vocab size: 3702\n",
      "concept size: 5\n",
      "load vocab from E:\\Workustc\\EduNLP\\workMaster\\EduNLP/examples/test_model/data\\vocab.list\n",
      "load concept from E:\\Workustc\\EduNLP\\workMaster\\EduNLP/examples/test_model/data\\concept.list\n",
      "load word2vec from E:\\Workustc\\EduNLP\\workMaster\\EduNLP/examples/test_model/data\\wv.th\n",
      "processing raw data for QuestionDataset...\n",
      "Start training the disenQNet...\n",
      "[Epoch  1] train loss: 1.7298\n",
      "[Epoch  2] train loss: 1.5702\n",
      "[Epoch  3] train loss: 1.4829\n",
      "[Epoch  4] train loss: 1.4349\n",
      "[Epoch  5] train loss: 1.3843\n",
      "[Epoch  6] train loss: 1.5143, eval loss: 1.5105\n",
      "[Epoch  7] train loss: 1.4440, eval loss: 1.4501\n",
      "[Epoch  8] train loss: 1.3966, eval loss: 1.4040\n",
      "[Epoch  9] train loss: 1.3692, eval loss: 1.3860\n",
      "[Epoch 10] train loss: 1.2979, eval loss: 1.3374\n",
      "[Epoch 11] train loss: 1.2851, eval loss: 1.3238\n",
      "[Epoch 12] train loss: 1.2696, eval loss: 1.3372\n",
      "[Epoch 13] train loss: 1.2296, eval loss: 1.3073\n",
      "[Epoch 14] train loss: 1.2304, eval loss: 1.3180\n",
      "[Epoch 15] train loss: 1.2058, eval loss: 1.2949\n",
      "[Epoch 16] train loss: 1.1966, eval loss: 1.2859\n",
      "[Epoch 17] train loss: 1.1709, eval loss: 1.2873\n",
      "[Epoch 18] train loss: 1.1434, eval loss: 1.2691\n",
      "[Epoch 19] train loss: 1.1431, eval loss: 1.2765\n",
      "[Epoch 20] train loss: 1.1141, eval loss: 1.2555\n",
      "[Epoch 21] train loss: 1.1357, eval loss: 1.2719\n",
      "[Epoch 22] train loss: 1.0926, eval loss: 1.2536\n",
      "[Epoch 23] train loss: 1.0569, eval loss: 1.2236\n",
      "[Epoch 24] train loss: 1.0450, eval loss: 1.2084\n",
      "[Epoch 25] train loss: 1.0747, eval loss: 1.2524\n",
      "[Epoch 26] train loss: 1.0421, eval loss: 1.2130\n",
      "[Epoch 27] train loss: 1.0004, eval loss: 1.1991\n",
      "[Epoch 28] train loss: 0.9989, eval loss: 1.2009\n",
      "[Epoch 29] train loss: 1.0193, eval loss: 1.2238\n",
      "[Epoch 30] train loss: 1.0346, eval loss: 1.2288\n",
      "[Epoch 31] train loss: 1.0046, eval loss: 1.2108\n",
      "[Epoch 32] train loss: 0.9964, eval loss: 1.2068\n",
      "[Epoch 33] train loss: 1.0088, eval loss: 1.2219\n",
      "[Epoch 34] train loss: 1.0024, eval loss: 1.2136\n",
      "[Epoch 35] train loss: 0.9980, eval loss: 1.2103\n",
      "[Epoch 36] train loss: 0.9638, eval loss: 1.1954\n",
      "[Epoch 37] train loss: 1.0003, eval loss: 1.2184\n",
      "[Epoch 38] train loss: 0.9956, eval loss: 1.2291\n",
      "[Epoch 39] train loss: 1.0310, eval loss: 1.2587\n",
      "[Epoch 40] train loss: 0.9344, eval loss: 1.1719\n",
      "[Epoch 41] train loss: 0.9384, eval loss: 1.1844\n",
      "[Epoch 42] train loss: 0.9493, eval loss: 1.2074\n",
      "[Epoch 43] train loss: 0.9680, eval loss: 1.2103\n",
      "[Epoch 44] train loss: 0.9499, eval loss: 1.1993\n",
      "[Epoch 45] train loss: 0.9158, eval loss: 1.1651\n",
      "[Epoch 46] train loss: 0.9564, eval loss: 1.2140\n",
      "[Epoch 47] train loss: 0.9469, eval loss: 1.1995\n",
      "[Epoch 48] train loss: 0.9734, eval loss: 1.2253\n",
      "[Epoch 49] train loss: 0.9890, eval loss: 1.2538\n",
      "[Epoch 50] train loss: 0.9709, eval loss: 1.2292\n",
      "[Epoch 51] train loss: 0.9525, eval loss: 1.2303\n",
      "[Epoch 52] train loss: 0.9674, eval loss: 1.2300\n",
      "[Epoch 53] train loss: 0.9355, eval loss: 1.2149\n",
      "[Epoch 54] train loss: 0.9315, eval loss: 1.2020\n",
      "[Epoch 55] train loss: 0.9666, eval loss: 1.2412\n"
     ]
    }
   ],
   "source": [
    "train_params = {\n",
    "    'epoch': 50,\n",
    "    'batch': 256,\n",
    "    'trim_min': 5,\n",
    "    'max_len': 250,\n",
    "    \"hidden\": 128,\n",
    "    'warm_up': 5,\n",
    "    \"device\": \"cuda\"\n",
    "}\n",
    "\n",
    "train_disenQNet(\n",
    "    disen_data_train,\n",
    "    output_dir,\n",
    "    output_dir,\n",
    "    train_params=train_params,\n",
    "    test_items=disen_data_test,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.使用模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 128])\n",
      "torch.Size([1, 23, 128])\n"
     ]
    }
   ],
   "source": [
    "tokenizer_kwargs = {\n",
    "    \"tokenizer_config_dir\": output_dir,\n",
    "}\n",
    "i2v = DisenQ('disenQ', 'disenq', output_dir, tokenizer_kwargs=tokenizer_kwargs, device=\"cuda\")\n",
    "\n",
    "test_items = [\n",
    "    {\"content\": \"10 米 的 (2/5) = 多少 米 的 (1/2),有 公 式\"},\n",
    "    {\"content\": \"10 米 的 (2/5) = 多少 米 的 (1/2),有 公 式 , 如 图 , 若 $x,y$ 满 足 约 束 条 件 公 式\"},\n",
    "]\n",
    "\n",
    "t_vec = i2v.infer_token_vector([test_items[1]], key=lambda x: x[\"content\"])\n",
    "i_vec = i2v.infer_item_vector(test_items[1], key=lambda x: x[\"content\"], vector_type=\"k\")\n",
    "\n",
    "print(i_vec.shape) # == torch.Size([x, 128])\n",
    "print(t_vec.shape) # == torch.Size([x, 150, 128])\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "776957673adb719a00031a24ed5efd2fa5ce8a13405e5193f8d278edd3805d55"
  },
  "kernelspec": {
   "display_name": "Python 3.6.13 ('data')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
