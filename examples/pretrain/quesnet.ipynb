{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\MySoftwares\\Anaconda\\envs\\data\\lib\\site-packages\\gensim\\similarities\\__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "import codecs\n",
    "from EduNLP.Pretrain import QuesNetTokenizer, pretrain_quesnet\n",
    "from EduNLP.Vector import T2V\n",
    "from EduNLP.I2V import QuesNet, get_pretrained_i2v\n",
    "\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 训练自己的QuesNet模型\n",
    "## 1. 数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设置你的数据路径和输出路径\n",
    "BASE_DIR = \"../..\"\n",
    "\n",
    "data_dir = f\"{BASE_DIR}/static/test_data\"\n",
    "output_dir = f\"{BASE_DIR}/examples/test_model/quesnet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def raw_data():\n",
    "    _data = []\n",
    "    data_path = os.path.join(data_dir, \"quesnet_data.json\")\n",
    "    with codecs.open(data_path, encoding=\"utf-8\") as f:\n",
    "        for line in f.readlines():\n",
    "            _data.append(json.loads(line))\n",
    "    return _data\n",
    "\n",
    "raw_data = raw_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 训练Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save words(3): 64/249 = 0.2570                  with frequency 696/927=0.7508\n",
      "save meta information know_name: 48\n",
      "vocab_size:  67\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = QuesNetTokenizer(meta=['know_name'], max_length=50,\n",
    "                             img_dir=os.path.join(data_dir, \"quesnet_img\"))\n",
    "\n",
    "# 设置词表\n",
    "tokenizer.set_vocab(raw_data, key=lambda x: x['ques_content'], trim_min_count=3, silent=False)\n",
    "\n",
    "print(\"vocab_size: \", tokenizer.vocab_size)\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存tokenizer\n",
    "tokenizer.save_pretrained(output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 训练QuesNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 自定义训练参数\n",
    "train_params = {\n",
    "    # train params\n",
    "    \"n_epochs\": 1,\n",
    "    \"batch_size\": 1,\n",
    "    \"lr\": 1e-3,\n",
    "    'save_every': 1,\n",
    "    'log_steps': 10,\n",
    "    # 'device': 'cpu',\n",
    "    'max_steps': 2,\n",
    "    # model params\n",
    "    'emb_size': 256,\n",
    "    'feat_size': 256,\n",
    "}\n",
    "\n",
    "# 当前仅支持linux下训练\n",
    "# pretrain_quesnet(os.path.join(os.path.abspath(data_dir), 'quesnet_data.json'),\n",
    "#                  output_dir, tokenizer, True, train_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 使用模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrain_dir = os.path.join(output_dir, \"quesnet_test_256\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 使用训练好的QuesNet Tokenzier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取保存的tokenizer\n",
    "tokenizer = QuesNetTokenizer.from_pretrained(pretrain_dir,\n",
    "                                             img_dir=os.path.join(data_dir, \"quesnet_img\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['已知', '集合', 'A', '=', '\\\\left', '\\\\{', 'x', '\\\\mid', 'x', '^', '{', '2', '}', '-', '3', 'x', '-', '4', '<', '0', '\\\\right', '\\\\}', ',', '\\\\quad', 'B', '=', '\\\\{', '-', '4', ',', '1', ',', '3', ',', '5', '\\\\}', ',', '\\\\quad', 'A', '\\\\cap', 'B', '=']\n",
      "\n",
      "[['已知', '集合', 'A', '=', '\\\\left', '\\\\{', 'x', '\\\\mid', 'x', '^', '{', '2', '}', '-', '3', 'x', '-', '4', '<', '0', '\\\\right', '\\\\}', ',', '\\\\quad', 'B', '=', '\\\\{', '-', '4', ',', '1', ',', '3', ',', '5', '\\\\}', ',', '\\\\quad', 'A', '\\\\cap', 'B', '='], ['复数', 'z', '=', '1', '+', '2', 'i', '+', 'i', '^', '{', '3', '}', '|', 'z', '|', '='], ['埃及', '胡夫', '金字塔', '古代', '世界', '建筑', '奇迹', '形状', '视为', '正四', '棱锥', '以该', '四', '棱锥', '高为', '边长', '正方形', '面积', '等于', '四', '棱锥', '侧面', '三角形', '面积', '侧面', '三角形', '底边', '高', '底面', '正方形', '边长', '比值'], ['设', 'O', '正方形', 'ABCD', '中心', 'O', ',', 'A', ',', 'B', ',', 'C', ',', 'D', '中任取', '3', '点', '取到', '3', '点', '共线', '概率'], ['某校', '课外', '学习', '小组', '研究', '作物', '发芽率', 'y', '温度', 'x', '单位', '^', '{', '\\\\circ', '}', '\\\\mathrm', '{', 'C', '}', '关系', '20', '温度', '条件', '种子', '发芽', '实验', '实验', '数据', '\\\\left', '(', 'x', '_', '{', 'i', '}', ',', 'y', '_', '{', 'i', '}', '\\\\right', ')', '(', 'i', '=', '1', ',', '2', ',', '\\\\cdots', ',', '20', ')', '散点图', \\FigureID{000004d6-0479-11ec-829b-797d5eb43535}, '散点图', '10', '^', '{', '\\\\circ', '}', '\\\\mathrm', '{', 'C', '}', '40', '^', '{', '\\\\circ', '}', '\\\\mathrm', '{', 'C', '}', '之间', '四个', '回归方程', '类型', '中', '适宜', '发芽率', 'y', '温度', 'x', '回归方程', '类型']]\n",
      "\n",
      "{'content_idx': [0, 0, 0, 14, 21, 0, 32, 0, 32, 27, 34, 10, 35, 7, 0, 32, 7, 0, 0, 0, 25, 0, 6, 0, 0, 14, 0, 7, 0, 6, 8, 6, 0, 6, 0, 0, 6, 0, 0, 0, 0, 14, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], 'meta_idx': {'know_name': [0, 0, 0]}, 'content': ['已知', '集合', 'A', '=', '\\\\left', '\\\\{', 'x', '\\\\mid', 'x', '^', '{', '2', '}', '-', '3', 'x', '-', '4', '<', '0', '\\\\right', '\\\\}', ',', '\\\\quad', 'B', '=', '\\\\{', '-', '4', ',', '1', ',', '3', ',', '5', '\\\\}', ',', '\\\\quad', 'A', '\\\\cap', 'B', '='], 'meta': {'know_name': ['代数', '集合', '集合的相等']}}\n",
      "\n",
      "{'content_idx': [[0, 0, 0, 14, 21, 0, 32, 0, 32, 27, 34, 10, 35, 7, 0, 32, 7, 0, 0, 0, 25, 0, 6, 0, 0, 14, 0, 7, 0, 6, 8, 6, 0, 6, 0, 0, 6, 0, 0, 0, 0, 14, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], [0, 0, 14, 8, 5, 10, 30, 5, 30, 27, 34, 0, 35, 0, 0, 0, 14, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], [56, 81, 88, 48, 37, 63, 57, 64, 82, 72, 71, 40, 51, 71, 91, 86, 73, 89, 79, 51, 71, 42, 36, 89, 42, 36, 61, 90, 62, 73, 86, 74, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]], 'meta_idx': [{'know_name': [0, 0, 0]}, {'know_name': [0, 0, 0]}, {'know_name': [8, 7, 5]}]}\n"
     ]
    }
   ],
   "source": [
    "# tokenize\n",
    "# 可以处理单个题目\n",
    "print(tokenizer.tokenize(raw_data[0], key=lambda x: x['ques_content']))\n",
    "print()\n",
    "# 也可以处理题目列表\n",
    "print(tokenizer.tokenize(raw_data[:5], key=lambda x: x['ques_content']))\n",
    "\n",
    "print()\n",
    "\n",
    "# 将token转换为index\n",
    "print(tokenizer(raw_data[0], key=lambda x: x['ques_content'], return_text=True, padding=True))\n",
    "print()\n",
    "print(tokenizer(raw_data[:3], key=lambda x: x['ques_content'], padding=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 使用训练好的QuesNet模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_kwargs = {\n",
    "    'tokenizer_config_dir': pretrain_dir,\n",
    "}\n",
    "i2v = QuesNet('quesnet', 'quesnet', pretrain_dir,\n",
    "              tokenizer_kwargs=tokenizer_kwargs, device=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 256])\n",
      "torch.Size([1, 43, 256])\n",
      "\n",
      "torch.Size([1, 43, 256])\n",
      "torch.Size([1, 256])\n",
      "\n",
      "torch.Size([2, 43, 256])\n",
      "torch.Size([2, 256])\n"
     ]
    }
   ],
   "source": [
    "# 获得单个题目的表征\n",
    "i_vec, t_vec = i2v(raw_data[0], key=lambda x: x[\"ques_content\"])\n",
    "print(i_vec.shape)\n",
    "print(t_vec.shape)\n",
    "print()\n",
    "\n",
    "# 也可以分别获得题目表征和各个token的表征\n",
    "t_vec = i2v.infer_token_vector(raw_data[0], key=lambda x: x[\"ques_content\"])\n",
    "i_vec = i2v.infer_item_vector(raw_data[0], key=lambda x: x[\"ques_content\"])\n",
    "print(t_vec.shape)\n",
    "print(i_vec.shape)\n",
    "print()\n",
    "\n",
    "# 获得题目列表的表征\n",
    "t_vec = i2v.infer_token_vector(raw_data[:2], key=lambda x: x[\"ques_content\"])\n",
    "i_vec = i2v.infer_item_vector(raw_data[:2], key=lambda x: x[\"ques_content\"])\n",
    "print(t_vec.shape)\n",
    "print(i_vec.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 使用EduNLP中公开的预训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EduNLP, INFO model_path: ..\\..\\examples\\test_model\\quesnet\\quesnet_test_256\n",
      "EduNLP, INFO Use pretrained t2v model quesnet_test_256\n",
      "downloader, INFO http://base.ustc.edu.cn/data/model_zoo/modelhub/quesnet_pub/1/quesnet_test_256.zip is saved as ..\\..\\examples\\test_model\\quesnet\\quesnet_test_256.zip\n",
      "downloader, INFO file existed, skipped\n"
     ]
    }
   ],
   "source": [
    "# 获取公开的预训练模型\n",
    "i2v = get_pretrained_i2v(\"quesnet_test_256\", model_dir=output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 256])\n",
      "torch.Size([1, 43, 256])\n",
      "\n",
      "torch.Size([1, 43, 256])\n",
      "torch.Size([1, 256])\n",
      "\n",
      "torch.Size([2, 43, 256])\n",
      "torch.Size([2, 256])\n"
     ]
    }
   ],
   "source": [
    "# 用法和I2V相同\n",
    "\n",
    "# 获得单个题目的表征\n",
    "i_vec, t_vec = i2v(raw_data[0], key=lambda x: x[\"ques_content\"])\n",
    "print(i_vec.shape)\n",
    "print(t_vec.shape)\n",
    "print()\n",
    "\n",
    "# 也可以分别获得题目表征和各个token的表征\n",
    "t_vec = i2v.infer_token_vector(raw_data[0], key=lambda x: x[\"ques_content\"])\n",
    "i_vec = i2v.infer_item_vector(raw_data[0], key=lambda x: x[\"ques_content\"])\n",
    "print(t_vec.shape)\n",
    "print(i_vec.shape)\n",
    "print()\n",
    "\n",
    "# 获得题目列表的表征\n",
    "t_vec = i2v.infer_token_vector(raw_data[:2], key=lambda x: x[\"ques_content\"])\n",
    "i_vec = i2v.infer_item_vector(raw_data[:2], key=lambda x: x[\"ques_content\"])\n",
    "print(t_vec.shape)\n",
    "print(i_vec.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2a09bcfc86f5d80d5adfb774779878f28f4d48d5a6d6c0020bcfd8afaf909ec6"
  },
  "kernelspec": {
   "display_name": "Python 3.6.13 ('data')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
