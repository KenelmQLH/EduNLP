{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "from transformers import BertModel as HFBertModel\n",
    "from EduNLP.Pretrain import BertTokenizer, finetune_bert\n",
    "from EduNLP.Vector import T2V, BertModel\n",
    "from EduNLP.I2V import Bert, get_pretrained_i2v\n",
    "\n",
    "import os\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 训练自己的 Bert 模型\n",
    "## 1. 数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设置你的数据路径和输出路径\n",
    "BASE_DIR = \"../..\"\n",
    "\n",
    "data_dir = f\"{BASE_DIR}/static/test_data\"\n",
    "output_dir = f\"{BASE_DIR}/examples/test_model/data/pretrain_test_models/bert\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem_data():\n",
    "    _data = []\n",
    "    data_path = os.path.join(data_dir, \"standard_luna_data.json\")\n",
    "    with open(data_path, encoding=\"utf-8\") as f:\n",
    "        for line in f.readlines():\n",
    "            _data.append(json.loads(line))\n",
    "    return _data\n",
    "\n",
    "train_items = stem_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 训练和评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'BertTokenizer'. \n",
      "The class this function is called from is 'EduTokenizerForBert'.\n",
      "Some weights of the model checkpoint at bert-base-chinese were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a529272c41a049ac8d24313d8023a09b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "/home/qlh/anaconda3/envs/dev/lib/python3.6/site-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n",
      "***** Running training *****\n",
      "  Num examples = 25\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 2\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 13\n",
      "/home/qlh/anaconda3/envs/dev/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:65: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13/13 00:02, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>4.236200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>3.176300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1.507000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>1.878000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Saving model checkpoint to ../../examples/test_model/data/pretrain_test_models/bert\n",
      "Configuration saved in ../../examples/test_model/data/pretrain_test_models/bert/config.json\n",
      "Model weights saved in ../../examples/test_model/data/pretrain_test_models/bert/pytorch_model.bin\n",
      "tokenizer config file saved in ../../examples/test_model/data/pretrain_test_models/bert/tokenizer_config.json\n",
      "Special tokens file saved in ../../examples/test_model/data/pretrain_test_models/bert/special_tokens_map.json\n",
      "added tokens file saved in ../../examples/test_model/data/pretrain_test_models/bert/added_tokens.json\n"
     ]
    }
   ],
   "source": [
    "# 自定义训练参数\n",
    "train_params = {\n",
    "  'num_train_epochs': 1,\n",
    "  'save_steps': 50,\n",
    "  'per_device_train_batch_size': 1,\n",
    "  'logging_steps': 3\n",
    "}\n",
    "\n",
    "finetune_bert(\n",
    "  train_items,\n",
    "  output_dir,\n",
    "  data_params={\n",
    "      \"stem_key\": \"ques_content\",\n",
    "  },\n",
    "  train_params=train_params\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.使用模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_items = [\n",
    "    {'ques_content': '有公式$\\\\FormFigureID{wrong1?}$和公式$\\\\FormFigureBase64{wrong2?}$，\\\n",
    "            如图$\\\\FigureID{088f15ea-8b7c-11eb-897e-b46bfc50aa29}$,\\\n",
    "            若$x,y$满足约束条件$\\\\SIFSep$，则$z=x+7 y$的最大值为$\\\\SIFBlank$'},\n",
    "    {'ques_content': '如图$\\\\FigureID{088f15ea-8b7c-11eb-897e-b46bfc50aa29}$, \\\n",
    "            若$x,y$满足约束条件$\\\\SIFSep$，则$z=x+7 y$的最大值为$\\\\SIFBlank$'}\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 直接加载令牌容器和模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file ../../examples/test_model/data/pretrain_test_models/bert/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-chinese\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 21137\n",
      "}\n",
      "\n",
      "loading weights file ../../examples/test_model/data/pretrain_test_models/bert/pytorch_model.bin\n",
      "Some weights of the model checkpoint at ../../examples/test_model/data/pretrain_test_models/bert were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertModel were not initialized from the model checkpoint at ../../examples/test_model/data/pretrain_test_models/bert and are newly initialized: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "loading file ../../examples/test_model/data/pretrain_test_models/bert/vocab.txt\n",
      "loading file ../../examples/test_model/data/pretrain_test_models/bert/added_tokens.json\n",
      "loading file ../../examples/test_model/data/pretrain_test_models/bert/special_tokens_map.json\n",
      "loading file ../../examples/test_model/data/pretrain_test_models/bert/tokenizer_config.json\n",
      "Adding [TEXT] to the vocabulary\n",
      "Adding [FORMULA] to the vocabulary\n",
      "Adding [FIGURE] to the vocabulary\n",
      "Adding [MARK] to the vocabulary\n",
      "Adding [TAG] to the vocabulary\n",
      "Adding [TEXT_BEGIN] to the vocabulary\n",
      "Adding [TEXT_END] to the vocabulary\n",
      "Adding [FORMULA_BEGIN] to the vocabulary\n",
      "Adding [FORMULA_END] to the vocabulary\n",
      "Assigning ['[TEXT]', '[FORMULA]', '[FIGURE]', '[MARK]', '[TAG]', '[SEP]', '[TEXT_BEGIN]', '[TEXT_END]', '[FORMULA_BEGIN]', '[FORMULA_END]'] to the additional_special_tokens key of the tokenizer\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BaseModelOutputWithPoolingAndCrossAttentions([('last_hidden_state',\n",
       "                                               tensor([[[ 0.4089,  1.0626,  0.0125,  ...,  0.9473, -1.1455, -0.1160],\n",
       "                                                        [-0.5247,  0.2449, -0.2175,  ..., -0.0708, -0.4598, -0.3746],\n",
       "                                                        [-0.5054,  0.5353, -0.9247,  ...,  1.1696,  0.0792, -0.3338],\n",
       "                                                        ...,\n",
       "                                                        [-0.1391,  0.0392, -0.8276,  ...,  0.9213, -0.1554, -0.2917],\n",
       "                                                        [ 0.5237,  0.2678,  0.6923,  ...,  0.1681, -0.9408, -0.2269],\n",
       "                                                        [ 0.3768,  0.2677,  0.3381,  ...,  0.9611, -2.1952, -0.0641]],\n",
       "                                               \n",
       "                                                       [[ 0.3635,  1.0077,  0.0537,  ...,  0.8781, -1.2010, -0.1730],\n",
       "                                                        [-0.4258,  0.3437, -0.1443,  ..., -0.0933, -0.3453, -0.3237],\n",
       "                                                        [ 0.1931, -0.2688,  0.8572,  ...,  1.2704, -0.6482, -0.4281],\n",
       "                                                        ...,\n",
       "                                                        [ 0.4101,  0.1993,  0.5072,  ...,  0.8726, -2.0718, -0.1272],\n",
       "                                                        [ 0.6080,  0.2398,  0.9711,  ...,  0.4306, -1.1894, -0.3648],\n",
       "                                                        [ 0.2173,  0.1151,  1.1694,  ...,  0.6153, -1.1397, -0.2648]]],\n",
       "                                                      grad_fn=<NativeLayerNormBackward>)),\n",
       "                                              ('pooler_output',\n",
       "                                               tensor([[ 0.4122, -0.3051, -0.0791,  ...,  0.3698, -0.4794, -0.4627],\n",
       "                                                       [ 0.4386, -0.2620, -0.0524,  ...,  0.3713, -0.4795, -0.3963]],\n",
       "                                                      grad_fn=<TanhBackward>))])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretrained_model_dir = output_dir\n",
    "\n",
    "model = HFBertModel.from_pretrained(pretrained_model_dir)\n",
    "tokenizer = BertTokenizer.from_pretrained(pretrained_model_dir)\n",
    "\n",
    "encodes = tokenizer(test_items[0], lambda x: x['ques_content'])\n",
    "model(**encodes)\n",
    "encodes = tokenizer(test_items, lambda x: x['ques_content'])\n",
    "model(**encodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file ../../examples/test_model/data/pretrain_test_models/bert/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-chinese\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 21137\n",
      "}\n",
      "\n",
      "loading weights file ../../examples/test_model/data/pretrain_test_models/bert/pytorch_model.bin\n",
      "Some weights of the model checkpoint at ../../examples/test_model/data/pretrain_test_models/bert were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertModel were not initialized from the model checkpoint at ../../examples/test_model/data/pretrain_test_models/bert and are newly initialized: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "loading file ../../examples/test_model/data/pretrain_test_models/bert/vocab.txt\n",
      "loading file ../../examples/test_model/data/pretrain_test_models/bert/added_tokens.json\n",
      "loading file ../../examples/test_model/data/pretrain_test_models/bert/special_tokens_map.json\n",
      "loading file ../../examples/test_model/data/pretrain_test_models/bert/tokenizer_config.json\n",
      "Adding [TEXT] to the vocabulary\n",
      "Adding [FORMULA] to the vocabulary\n",
      "Adding [FIGURE] to the vocabulary\n",
      "Adding [MARK] to the vocabulary\n",
      "Adding [TAG] to the vocabulary\n",
      "Adding [TEXT_BEGIN] to the vocabulary\n",
      "Adding [TEXT_END] to the vocabulary\n",
      "Adding [FORMULA_BEGIN] to the vocabulary\n",
      "Adding [FORMULA_END] to the vocabulary\n",
      "Assigning ['[TEXT]', '[FORMULA]', '[FIGURE]', '[MARK]', '[TAG]', '[SEP]', '[TEXT_BEGIN]', '[TEXT_END]', '[FORMULA_BEGIN]', '[FORMULA_END]'] to the additional_special_tokens key of the tokenizer\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 768])\n",
      "torch.Size([1, 17, 768])\n",
      "torch.Size([2, 768])\n",
      "torch.Size([2, 17, 768])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tokenizer_kwargs = {\"tokenizer_config_dir\": output_dir}\n",
    "i2v = Bert('bert', 'bert', output_dir, tokenizer_kwargs=tokenizer_kwargs)\n",
    "\n",
    "# 可以对单个题目进行表征\n",
    "i_vec, t_vec = i2v(test_items[0], key=lambda x: x[\"ques_content\"])\n",
    "print(i_vec.shape) # == torch.Size([x])\n",
    "print(t_vec.shape) # == torch.Size([x, x])\n",
    "\n",
    "# 也可以对题目列表进行表征\n",
    "i_vec, t_vec = i2v(test_items, key=lambda x: x[\"ques_content\"])\n",
    "print(i_vec.shape) # == torch.Size([2, x])\n",
    "print(t_vec.shape) # == torch.Size([2, x, x]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 使用 BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file ../../examples/test_model/data/pretrain_test_models/bert/vocab.txt\n",
      "loading file ../../examples/test_model/data/pretrain_test_models/bert/added_tokens.json\n",
      "loading file ../../examples/test_model/data/pretrain_test_models/bert/special_tokens_map.json\n",
      "loading file ../../examples/test_model/data/pretrain_test_models/bert/tokenizer_config.json\n",
      "Adding [TEXT] to the vocabulary\n",
      "Adding [FORMULA] to the vocabulary\n",
      "Adding [FIGURE] to the vocabulary\n",
      "Adding [MARK] to the vocabulary\n",
      "Adding [TAG] to the vocabulary\n",
      "Adding [TEXT_BEGIN] to the vocabulary\n",
      "Adding [TEXT_END] to the vocabulary\n",
      "Adding [FORMULA_BEGIN] to the vocabulary\n",
      "Adding [FORMULA_END] to the vocabulary\n",
      "Assigning ['[TEXT]', '[FORMULA]', '[FIGURE]', '[MARK]', '[TAG]', '[SEP]', '[TEXT_BEGIN]', '[TEXT_END]', '[FORMULA_BEGIN]', '[FORMULA_END]'] to the additional_special_tokens key of the tokenizer\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  101,   100,   100,   100, 21130,   166,   117,   167,   100,   102,\n",
      "           168,   134,   166,   116,   128,   167,   100, 21131,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "\n",
      "{'input_ids': tensor([[  101,   100,   100,   100, 21130,   166,   117,   167,   100,   102,\n",
      "           168,   134,   166,   116,   128,   167,   100, 21131,   102],\n",
      "        [  101,   100, 21130,   166,   117,   167,   100,   102,   168,   134,\n",
      "           166,   116,   128,   167,   100, 21131,   102,     0,     0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0]])}\n",
      "\n",
      "{'input_ids': tensor([[  101,   100,   100,   100, 21130,   166,   117,   167,   100,   102,\n",
      "           168,   134,   166,   116,   128,   167,   100, 21131,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "['公式', '公式', '如图', '[FIGURE]', 'x', ',', 'y', '约束条件', '[SEP]', 'z', '=', 'x', '+', '7', 'y', '最大值', '[MARK]']\n",
      "[['公式', '公式', '如图', '[FIGURE]', 'x', ',', 'y', '约束条件', '[SEP]', 'z', '=', 'x', '+', '7', 'y', '最大值', '[MARK]'], ['如图', '[FIGURE]', 'x', ',', 'y', '约束条件', '[SEP]', 'z', '=', 'x', '+', '7', 'y', '最大值', '[MARK]']]\n"
     ]
    }
   ],
   "source": [
    "# 在 Bert-base-chinese 的基础上初始化 tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(output_dir)\n",
    "\n",
    "\n",
    "# 可以对单个题目进行令牌化\n",
    "print(tokenizer(test_items[0], key=lambda x: x['ques_content']))\n",
    "print()\n",
    "\n",
    "# 也可以对题目列表进行令牌化\n",
    "token_items = tokenizer(test_items, key=lambda x: x['ques_content'])\n",
    "print(token_items)\n",
    "print()\n",
    "\n",
    "# 可以使用 return_tensors 参数指定返回张量的类型\n",
    "print(tokenizer(test_items[0], key=lambda x: x['ques_content'], return_tensors='pt'))\n",
    "\n",
    "\n",
    "# 可以使用 tokenize 方法查看令牌化后的文本\n",
    "print(tokenizer.tokenize(test_items[0], key=lambda x: x['ques_content']))\n",
    "print(tokenizer.tokenize(test_items, key=lambda x: x['ques_content']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file ../../examples/test_model/data/pretrain_test_models/bert/vocab.txt\n",
      "loading file ../../examples/test_model/data/pretrain_test_models/bert/added_tokens.json\n",
      "loading file ../../examples/test_model/data/pretrain_test_models/bert/special_tokens_map.json\n",
      "loading file ../../examples/test_model/data/pretrain_test_models/bert/tokenizer_config.json\n",
      "Adding [TEXT] to the vocabulary\n",
      "Adding [FORMULA] to the vocabulary\n",
      "Adding [FIGURE] to the vocabulary\n",
      "Adding [MARK] to the vocabulary\n",
      "Adding [TAG] to the vocabulary\n",
      "Adding [TEXT_BEGIN] to the vocabulary\n",
      "Adding [TEXT_END] to the vocabulary\n",
      "Adding [FORMULA_BEGIN] to the vocabulary\n",
      "Adding [FORMULA_END] to the vocabulary\n",
      "Assigning ['[TEXT]', '[FORMULA]', '[FIGURE]', '[MARK]', '[TAG]', '[SEP]', '[TEXT_BEGIN]', '[TEXT_END]', '[FORMULA_BEGIN]', '[FORMULA_END]'] to the additional_special_tokens key of the tokenizer\n",
      "loading configuration file ../../examples/test_model/data/pretrain_test_models/bert/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-chinese\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 21137\n",
      "}\n",
      "\n",
      "loading weights file ../../examples/test_model/data/pretrain_test_models/bert/pytorch_model.bin\n",
      "Some weights of the model checkpoint at ../../examples/test_model/data/pretrain_test_models/bert were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertModel were not initialized from the model checkpoint at ../../examples/test_model/data/pretrain_test_models/bert and are newly initialized: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 19, 768])\n",
      "\n",
      "torch.Size([2, 768])\n",
      "torch.Size([2, 17, 768])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 加载之前训练的模型 tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(pretrained_model_dir)\n",
    "encodes = tokenizer(test_items, key=lambda x: x['ques_content'])\n",
    "\n",
    "t2v = BertModel(pretrained_model_dir)\n",
    "i_vec = t2v(encodes)\n",
    "print(i_vec.shape) # == torch.Size([2, x])\n",
    "print()\n",
    "\n",
    "i_vec = t2v.infer_vector(encodes)\n",
    "t_vec = t2v.infer_tokens(encodes)\n",
    "print(i_vec.shape) # == torch.Size([2, x])\n",
    "print(t_vec.shape) # == torch.Size([2, x, x]))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 使用 EduNLP 中公开的预训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EduNLP, INFO model_path: ..\\..\\examples\\test_model/data\\data\\bert\\LUNABert\n",
      "EduNLP, INFO Use pretrained t2v model luna_bert\n",
      "downloader, INFO http://base.ustc.edu.cn/data/model_zoo/EduNLP/LUNABert.zip is saved as ..\\..\\examples\\test_model/data\\data\\bert\\LUNABert.zip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading ..\\..\\examples\\test_model/data\\data\\bert\\LUNABert.zip 100.00%: 362MB | 362MB"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "downloader, INFO ..\\..\\examples\\test_model/data\\data\\bert\\LUNABert.zip is unzip to ..\\..\\examples\\test_model/data\\data\\bert\\LUNABert\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file ..\\..\\examples\\test_model/data\\data\\bert\\LUNABert\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-chinese\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.11.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 21132\n",
      "}\n",
      "\n",
      "loading weights file ..\\..\\examples\\test_model/data\\data\\bert\\LUNABert\\pytorch_model.bin\n",
      "Some weights of the model checkpoint at ..\\..\\examples\\test_model/data\\data\\bert\\LUNABert were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertModel were not initialized from the model checkpoint at ..\\..\\examples\\test_model/data\\data\\bert\\LUNABert and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "loading configuration file ..\\..\\examples\\test_model/data\\data\\bert\\LUNABert\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-chinese\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.11.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 21132\n",
      "}\n",
      "\n",
      "Didn't find file ..\\..\\examples\\test_model/data\\data\\bert\\LUNABert\\tokenizer.json. We won't load it.\n",
      "loading file ..\\..\\examples\\test_model/data\\data\\bert\\LUNABert\\vocab.txt\n",
      "loading file None\n",
      "loading file ..\\..\\examples\\test_model/data\\data\\bert\\LUNABert\\added_tokens.json\n",
      "loading file ..\\..\\examples\\test_model/data\\data\\bert\\LUNABert\\special_tokens_map.json\n",
      "loading file ..\\..\\examples\\test_model/data\\data\\bert\\LUNABert\\tokenizer_config.json\n",
      "loading configuration file ..\\..\\examples\\test_model/data\\data\\bert\\LUNABert\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-chinese\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.11.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 21132\n",
      "}\n",
      "\n",
      "Adding [FORMULA] to the vocabulary\n",
      "Adding [FIGURE] to the vocabulary\n",
      "Adding [MARK] to the vocabulary\n",
      "Adding [TAG] to the vocabulary\n",
      "loading configuration file ..\\..\\examples\\test_model/data\\data\\bert\\LUNABert\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-chinese\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.11.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 21132\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 获取公开的预训练模型\n",
    "pretrained_dir = f\"{BASE_DIR}/examples/test_model/data/data/bert\"\n",
    "i2v = get_pretrained_i2v(\"luna_bert\", model_dir=pretrained_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 768])\n",
      "torch.Size([2, 32, 768])\n",
      "\n",
      "torch.Size([2, 768])\n",
      "torch.Size([2, 32, 768])\n",
      "\n",
      "torch.Size([1, 768])\n",
      "torch.Size([1, 2, 768])\n"
     ]
    }
   ],
   "source": [
    "i_vec, t_vec = i2v(test_items, key=lambda x: x['ques_content'])\n",
    "print(i_vec.shape)\n",
    "print(t_vec.shape)\n",
    "print()\n",
    "\n",
    "# 也可以单独获取题目表征和各个 token 的表征\n",
    "i_vec = i2v.infer_item_vector(test_items, key=lambda x: x['ques_content'])\n",
    "print(i_vec.shape)\n",
    "t_vec = i2v.infer_token_vector(test_items, key=lambda x: x['ques_content'])\n",
    "print(t_vec.shape)\n",
    "print()\n",
    "\n",
    "# 同样，可以获取单个题目的表征\n",
    "i_vec, t_vec = i2v(test_items[0], key=lambda x: x['ques_content'])\n",
    "print(i_vec.shape)\n",
    "print(t_vec.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2469a70536e4d2335a2ea8907942d0699c37342a371ac185bdb5b0aa6f073890"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
