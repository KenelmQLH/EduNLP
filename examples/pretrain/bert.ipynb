{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\MySoftwares\\Anaconda\\envs\\data\\lib\\site-packages\\gensim\\similarities\\__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "from EduNLP.Pretrain import BertTokenizer, finetune_bert\n",
    "from EduNLP.Vector import T2V\n",
    "from EduNLP.I2V import Bert, get_pretrained_i2v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 训练自己的Bert模型\n",
    "## 1. 数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = \"E:\\Workustc\\EduNLP\\workMaster\\EduNLP\"\n",
    "\n",
    "data_dir = f\"{BASE_DIR}/tests/test_vec/test_data\"\n",
    "output_dir = f\"{BASE_DIR}/examples/test_model/data/bert\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def raw_data():\n",
    "    _data = []\n",
    "    data_path = os.path.join(data_dir, \"OpenLUNA.json\")\n",
    "    with open(data_path, encoding=\"utf-8\") as f:\n",
    "        for line in f.readlines():\n",
    "            _data.append(json.loads(line))\n",
    "    return _data\n",
    "\n",
    "def stem_data(data):\n",
    "    _data = []\n",
    "    tokenizer = BertTokenizer()\n",
    "    for e in data:\n",
    "        d = tokenizer(e[\"stem\"])\n",
    "        if d is not None:\n",
    "            _data.append(d)\n",
    "    assert _data\n",
    "    return _data\n",
    "\n",
    "raw_data = raw_data()\n",
    "train_items = stem_data(raw_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 训练和评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_params = {\n",
    "  'epochs': 1,\n",
    "  'save_steps': 100,\n",
    "  'batch_size': 1,\n",
    "  'logging_steps': 3\n",
    "}\n",
    "\n",
    "\n",
    "finetune_bert(\n",
    "  train_items,\n",
    "  output_dir,\n",
    "  train_params=train_params\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.使用模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item = {'stem': '如图$\\\\FigureID{088f15ea-8b7c-11eb-897e-b46bfc50aa29}$, \\\n",
    "        若$x,y$满足约束条件$\\\\SIFSep$，则$z=x+7 y$的最大值为$\\\\SIFBlank$'}\n",
    "\n",
    "tokenizer_kwargs = {\"pretrain_model\": output_dir}\n",
    "i2v = Bert('bert', 'bert', output_dir, tokenizer_kwargs=tokenizer_kwargs)\n",
    "\n",
    "i_vec, t_vec = i2v(item['stem'])\n",
    "\n",
    "i_vec, t_vec = i2v([ item['stem'] ])\n",
    "# i_vec, t_vec = i2v([item['stem'],item2['stem']]) # same output!\n",
    "# or\n",
    "# i_vec = i2v.infer_item_vector([item['stem']])\n",
    "# t_vec = i2v.infer_token_vector([item['stem']])\n",
    "\n",
    "print(i_vec.shape) # == torch.Size([x, x])\n",
    "print(t_vec.shape) # == torch.Size([x, x, x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer()\n",
    "item = \"有公式$\\\\FormFigureID{wrong1?}$，如图$\\\\FigureID{088f15ea-xxx}$,\\\n",
    " 若$x,y$满足约束条件公式$\\\\FormFigureBase64{wrong2?}$,$\\\\SIFSep$，则$z=x+7 y$的最大值为$\\\\SIFBlank$\"\n",
    "token_item = tokenizer(item)\n",
    "print(token_item.input_ids[:10])\n",
    "[101, 1062, 2466, 1963, 1745, 21129, 166, 117, 167, 5276]\n",
    "print(tokenizer.tokenize(item)[:10])\n",
    "['公', '式', '如', '图', '[FIGURE]', 'x', ',', 'y', '约', '束']\n",
    "items = [item, item]\n",
    "token_items = tokenizer(items, return_tensors='pt')\n",
    "print(token_items.input_ids.shape)\n",
    "# torch.Size([2, 27])\n",
    "print(len(tokenizer.tokenize(items)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_item = tokenizer(item, return_tensors='pt')\n",
    "print(token_item.input_ids.shape)\n",
    "\n",
    "token_item = tokenizer(item)\n",
    "print(token_item.input_ids)\n",
    "\n",
    "print(len(tokenizer.tokenize(item)))\n",
    "\n",
    "print(tokenizer.tokenize(item))\n",
    "print(tokenizer.tokenize(items))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EduNLP, INFO model_dir: E:\\Workustc\\EduNLP\\workMaster\\EduNLP\\examples\\test_model\\data\\disenq\\disenq_pub_128\n",
      "EduNLP, INFO Use pretrained t2v model disenq_pub_128\n",
      "downloader, INFO http://base.ustc.edu.cn/data/model_zoo/modelhub/disenq_public/1/disenq_pub_128.zip is saved as E:\\Workustc\\EduNLP\\workMaster\\EduNLP\\examples\\test_model\\data\\disenq\\disenq_pub_128.zip\n",
      "downloader, INFO file existed, skipped\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token_items ['<num>', '米', '的', '<num>', '=', '多少', '米', '的', '(1/2),有', '公', '式']\n",
      "seqs [[0, 2805, 2598, 0, 10, 1147, 2805, 2598, 1, 1, 1553]]\n",
      "ret {'content_idx': [[0, 2805, 2598, 0, 10, 1147, 2805, 2598, 1, 1, 1553]], 'content_len': [11]}\n",
      "token_items ['<num>', '米', '的', '<num>', '=', '多少', '米', '的', '(1/2),有', '公', '式']\n",
      "seqs [[0, 2805, 2598, 0, 10, 1147, 2805, 2598, 1, 1, 1553]]\n",
      "ret {'content_idx': [[0, 2805, 2598, 0, 10, 1147, 2805, 2598, 1, 1, 1553]], 'content_len': [11]}\n",
      "token_items ['<num>', '米', '的', '<num>', '=', '多少', '米', '的', '(1/2),有', '公', '式']\n",
      "seqs [[0, 2805, 2598, 0, 10, 1147, 2805, 2598, 1, 1, 1553]]\n",
      "ret {'content_idx': [[0, 2805, 2598, 0, 10, 1147, 2805, 2598, 1, 1, 1553]], 'content_len': [11]}\n",
      "token_items ['<num>', '米', '的', '<num>', '=', '多少', '米', '的', '(1/2),有', '公', '式']\n",
      "seqs [[0, 2805, 2598, 0, 10, 1147, 2805, 2598, 1, 1, 1553]]\n",
      "ret {'content_idx': [[0, 2805, 2598, 0, 10, 1147, 2805, 2598, 1, 1, 1553]], 'content_len': [11]}\n",
      "token_items ['<num>', '米', '的', '<num>', '=', '多少', '米', '的', '(1/2),有', '公', '式']\n",
      "seqs [[0, 2805, 2598, 0, 10, 1147, 2805, 2598, 1, 1, 1553]]\n",
      "ret {'content_idx': [[0, 2805, 2598, 0, 10, 1147, 2805, 2598, 1, 1, 1553]], 'content_len': [11]}\n",
      "token_items [['<num>', '米', '的', '<num>', '=', '多少', '米', '的', '(1/2),有', '公', '式'], ['<num>', '米', '的', '<num>', '=', '多少', '米', '的', '(1/2),有', '公', '式', '如', '图', '若', '$x,y$', '满', '足', '约', '束', '条', '件', '公', '式']]\n",
      "seqs [[0, 2805, 2598, 0, 10, 1147, 2805, 2598, 1, 1, 1553], [0, 2805, 2598, 0, 10, 1147, 2805, 2598, 1, 1, 1553, 1249, 1063, 3020, 1, 2384, 1, 2840, 2027, 2028, 434, 1, 1553]]\n",
      "ret {'content_idx': [[0, 2805, 2598, 0, 10, 1147, 2805, 2598, 1, 1, 1553, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], [0, 2805, 2598, 0, 10, 1147, 2805, 2598, 1, 1, 1553, 1249, 1063, 3020, 1, 2384, 1, 2840, 2027, 2028, 434, 1, 1553]], 'content_len': [11, 23]}\n"
     ]
    }
   ],
   "source": [
    "test_items = [\n",
    "        {\"content\": \"10 米 的 (2/5) = 多少 米 的 (1/2),有 公 式\"},\n",
    "        {\"content\": \"10 米 的 (2/5) = 多少 米 的 (1/2),有 公 式 , 如 图 , 若 $x,y$ 满 足 约 束 条 件 公 式\"},\n",
    "    ]\n",
    "\n",
    "pretrained_dir = f\"{BASE_DIR}/examples/test_model/data/disenq\"\n",
    "i2v = get_pretrained_i2v(\"disenq_pub_128\", model_dir=pretrained_dir)\n",
    "i_vec, t_vec = i2v(test_items[0], key=lambda x: x[\"content\"])\n",
    "assert len(i_vec) == 2\n",
    "assert t_vec.shape[2] == i2v.vector_size\n",
    "\n",
    "t_vec = i2v.infer_token_vector(test_items[0], key=lambda x: x[\"content\"])\n",
    "i_vec_k = i2v.infer_item_vector(test_items[0], key=lambda x: x[\"content\"], vector_type=\"k\")\n",
    "i_vec_i = i2v.infer_item_vector(test_items[0], key=lambda x: x[\"content\"], vector_type=\"i\")\n",
    "assert i_vec_k.shape == torch.Size([1, 128])\n",
    "assert i_vec_i.shape == torch.Size([1, 128])\n",
    "assert t_vec.shape == torch.Size([1, 11, 128])\n",
    "assert i2v.vector_size == i_vec_k.shape[1]\n",
    "\n",
    "i_vec, t_vec = i2v.infer_vector(test_items[0], key=lambda x: x[\"content\"], vector_type=None)\n",
    "assert len(i_vec) == 2\n",
    "assert i_vec[0].shape == torch.Size([1, 128])\n",
    "assert i_vec[1].shape == torch.Size([1, 128])\n",
    "assert t_vec.shape == torch.Size([1, 11, 128])\n",
    "\n",
    "i_vec, t_vec = i2v(test_items, key=lambda x: x[\"content\"])\n",
    "assert t_vec.shape == torch.Size([2, 23, 128])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "776957673adb719a00031a24ed5efd2fa5ce8a13405e5193f8d278edd3805d55"
  },
  "kernelspec": {
   "display_name": "Python 3.6.13 ('data')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
