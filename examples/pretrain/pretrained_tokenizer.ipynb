{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/qlh/anaconda3/envs/py36/lib/python3.6/site-packages/gensim/similarities/__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "from EduNLP.Pretrain import PretrainedEduTokenizer, EduDataset\n",
    "import os\n",
    "import json\n",
    "\n",
    "\n",
    "BASE_DIR = \"../..\"\n",
    "data_dir = f\"{BASE_DIR}/static/test_data\"\n",
    "output_dir = f\"{BASE_DIR}/data/pretrain_test_models/pretrain/\"\n",
    "\n",
    "\n",
    "def stem_data():\n",
    "    _data = []\n",
    "    data_path = os.path.join(data_dir, \"standard_luna_data.json\")\n",
    "    with open(data_path, encoding=\"utf-8\") as f:\n",
    "        for line in f.readlines():\n",
    "            _data.append(json.loads(line))\n",
    "    return _data\n",
    "\n",
    "train_items = stem_data()\n",
    "\n",
    "test_items = [\n",
    "    {'ques_content': '有公式$\\\\FormFigureID{wrong1?}$和公式$\\\\FormFigureBase64{wrong2?}$，\\\n",
    "            如图$\\\\FigureID{088f15ea-8b7c-11eb-897e-b46bfc50aa29}$,\\\n",
    "            若$x,y$满足约束条件$\\\\SIFSep$，则$z=x+7 y$的最大值为$\\\\SIFBlank$'},\n",
    "    {'ques_content': '如图$\\\\FigureID{088f15ea-8b7c-11eb-897e-b46bfc50aa29}$, \\\n",
    "            若$x,y$满足约束条件$\\\\SIFSep$，则$z=x+7 y$的最大值为$\\\\SIFBlank$'}\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. PretrainedEduTokenizer\n",
    "\n",
    "该类主要用于处理预训练模型的输入语料，主要成分包括词表(vocab) 和 基础令牌话容器，负责将输入语料处理为适合模型的输入格式。\n",
    "\n",
    "## 1.1 构造令牌化容器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dump cache file failed.\n",
      "Traceback (most recent call last):\n",
      "  File \"/data/qlh/anaconda3/envs/py36/lib/python3.6/site-packages/jieba/__init__.py\", line 154, in initialize\n",
      "    _replace_file(fpath, cache_file)\n",
      "PermissionError: [Errno 1] Operation not permitted: '/tmp/tmpk245c2ok' -> '/tmp/jieba.cache'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "379\n"
     ]
    }
   ],
   "source": [
    "corpus_items = train_items + test_items\n",
    "\n",
    "# 定义参数\n",
    "tokenizer_params = {\n",
    "    \"add_specials\": True,\n",
    "    \"tokenize_method\": \"pure_text\",\n",
    "}\n",
    "# 可自定义pure_text的参数， 参考Tokenizer/PureTextTokenizer\n",
    "text_params = {\n",
    "    \"granularity\": \"char\",\n",
    "    \"stopwords\": None,\n",
    "}\n",
    "\n",
    "tokenizer = PretrainedEduTokenizer(**tokenizer_params, text_params=text_params)\n",
    "print(len(tokenizer))\n",
    "\n",
    "\n",
    "# 设置预训练语料，训练令牌话容器\n",
    "tokenizer.set_vocab(corpus_items, key=lambda x: x['ques_content'])\n",
    "print(len(tokenizer))\n",
    "\n",
    "# 保存令牌话容器\n",
    "pretrained_tokenizer_dir = output_dir\n",
    "tokenizer.save_pretrained(pretrained_tokenizer_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 使用令牌化容器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['seq_idx', 'seq_len']\n",
      "torch.Size([2, 17])\n",
      "\n",
      "['seq_idx', 'seq_len']\n",
      "torch.Size([2, 100])\n",
      "\n",
      "[[1, 1, 1, 6, 22, 35, 130, 1, 9, 45, 19, 22, 46, 211, 130, 1, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 6, 22, 35, 130, 1, 9, 45, 19, 22, 46, 211, 130, 1, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n",
      "\n",
      "['seq_idx', 'seq_len', 'seq_token']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 加载令牌话容器\n",
    "tokenizer = PretrainedEduTokenizer.from_pretrained(pretrained_tokenizer_dir)\n",
    "\n",
    "# 按batch进行padding\n",
    "encodes = tokenizer(test_items, key=lambda x: x['ques_content'])\n",
    "print(list(encodes.keys()))\n",
    "print(encodes[\"seq_idx\"].shape)\n",
    "print()\n",
    "\n",
    "# 按max_length进行padding\n",
    "encodes = tokenizer(test_items, key=lambda x: x['ques_content'], padding=\"max_length\", max_length=100)\n",
    "print(list(encodes.keys()))\n",
    "print(encodes[\"seq_idx\"].shape)\n",
    "print()\n",
    "\n",
    "# 不返回tensor\n",
    "encodes = tokenizer(test_items, key=lambda x: x['ques_content'], padding=\"max_length\", max_length=100, return_tensors=False)\n",
    "print(encodes[\"seq_idx\"])\n",
    "print()\n",
    "\n",
    "# 保留tokens\n",
    "encodes = tokenizer(test_items, key=lambda x: x['ques_content'], padding=\"max_length\", max_length=100, return_text=True)\n",
    "print(list(encodes.keys()))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 其他操作\n",
    "\n",
    "扩充词表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[PAD]', '[UNK]', '[BOS]', '[EOS]', '[TEXT]', '[FORMULA]', '[FIGURE]', '[MARK]', '[TAG]', '[SEP]', '[TEXT_BEGIN]', '[TEXT_END]', '[FORMULA_BEGIN]', '[FORMULA_END]', '[TEXT]', '[FORMULA]', '[FIGURE]', '[MARK]', '[TAG]', '[SEP]', '[TEXT_BEGIN]', '[TEXT_END]', '[FORMULA_BEGIN]', '[FORMULA_END]']\n",
      "\n",
      "['special']\n",
      "['[PAD]', '[UNK]', '[BOS]', '[EOS]', '[TEXT]', '[FORMULA]', '[FIGURE]', '[MARK]', '[TAG]', '[SEP]', '[TEXT_BEGIN]', '[TEXT_END]', '[FORMULA_BEGIN]', '[FORMULA_END]', '[TEXT]', '[FORMULA]', '[FIGURE]', '[MARK]', '[TAG]', '[SEP]', '[TEXT_BEGIN]', '[TEXT_END]', '[FORMULA_BEGIN]', '[FORMULA_END]', '[special]']\n",
      "\n",
      "['token']\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.vocab._special_tokens)\n",
    "print()\n",
    "\n",
    "# 增加特殊词\n",
    "tokenizer.add_specials([\"[special]\"])\n",
    "print(tokenizer.tokenize(\"[special]\"))\n",
    "print(tokenizer.vocab._special_tokens)\n",
    "print()\n",
    "\n",
    "# 增加词\n",
    "tokenizer.add_tokens([\"[token]\"])\n",
    "print(tokenizer.tokenize(\"[token]\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "编码/解码 句子"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 370, 371]\n",
      "['[UNK]', '公', '式']\n"
     ]
    }
   ],
   "source": [
    "encode_idxs = tokenizer.encode('公式 公 式')\n",
    "print(encode_idxs)\n",
    "\n",
    "encode_tokens = tokenizer.decode(encode_idxs)\n",
    "print(encode_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "修改基础令牌化容器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 可自定义参数\n",
    "formula_params = {\n",
    "    \"skip_figure_formula\": True,\n",
    "    \"symbolize_figure_formula\": False\n",
    "}\n",
    "\n",
    "tokenizer._set_basic_tokenizer(\"ast_formula\", formula_params=formula_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "保存与加载"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存\n",
    "save_dir = \"./tmp\"\n",
    "tokenizer.save_pretrained(save_dir)\n",
    "\n",
    "# 加载\n",
    "tokenizer = PretrainedEduTokenizer.from_pretrained(save_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EduDataset\n",
    "\n",
    "## 直接使用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef0459814fb544fba1012e98961c1035",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['seq_idx', 'seq_len'])\n"
     ]
    }
   ],
   "source": [
    "# 使用EduDataset\n",
    "dataset = EduDataset(tokenizer, items=train_items,\n",
    "                     stem_key=\"ques_content\")\n",
    "print(dataset[0].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "113cf20b74964d31ab2905dd22fccaa6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['labels', 'seq_idx', 'seq_len'])\n"
     ]
    }
   ],
   "source": [
    "dataset = EduDataset(tokenizer, items=train_items,\n",
    "                     stem_key=\"ques_content\", label_key=\"difficulty\")\n",
    "print(dataset[0].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4cb52b096604adc9315de47f8b08ff2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['know_list', 'labels', 'seq_idx', 'seq_len'])\n"
     ]
    }
   ],
   "source": [
    "dataset = EduDataset(tokenizer, items=train_items,\n",
    "                     stem_key=\"ques_content\", label_key=\"difficulty\", feature_keys=[\"know_list\"])\n",
    "print(dataset[0].keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 保存与加载\n",
    "\n",
    "考虑到预处理耗时久，若希望下次能直接使用处理后的数据，可将预处理后的数据保存在本地。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.to_disk(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['seq_idx', 'seq_len'])\n",
      "dict_keys(['know_list', 'labels', 'seq_idx', 'seq_len'])\n"
     ]
    }
   ],
   "source": [
    "# # 保存\n",
    "dataset.to_disk(output_dir)\n",
    "\n",
    "# # 加载\n",
    "dataset1 = EduDataset(tokenizer, ds_disk_path=output_dir)\n",
    "print(dataset1[0].keys())\n",
    "\n",
    "dataset2 = EduDataset(tokenizer, ds_disk_path=output_dir, label_key=\"difficulty\", feature_keys=[\"know_list\"])\n",
    "print(dataset2[0].keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 并行预处理\n",
    "在题目数据量过大时，令牌化等预处理操作耗时较长，可通过并行处理加速。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60db0b94003e4681b56d88465c6c7c39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#0:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5db7f48bf4ad43ea8e951fa3173d09b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#1:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05676952d6734c5f93a6618511f40f82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#2:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43cb913cefbc4e1ca22bbd5d92db9fa5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#3:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['seq_idx', 'seq_len'])\n",
      "spand time: 1.641s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e63e29b8163448d87343ea0cb89bf2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['seq_idx', 'seq_len'])\n",
      "spand time: 4.484s\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "s = time.time()\n",
    "# 使用并行加速\n",
    "dataset = EduDataset(tokenizer, items=train_items*100,\n",
    "                    stem_key=\"ques_content\",\n",
    "                    num_processor=4)\n",
    "print(dataset[0].keys())\n",
    "e = time.time()\n",
    "print(f\"spand time: {(e - s):.4}s\")\n",
    "\n",
    "s = time.time()\n",
    "# 不使用并行加速\n",
    "dataset = EduDataset(tokenizer, items=train_items*100,\n",
    "                    stem_key=\"ques_content\",)\n",
    "print(dataset[0].keys())\n",
    "e = time.time()\n",
    "print(f\"spand time: {(e - s):.4}s\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.13 ('py36')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "672c49ef5d0c797ca83477c465883c954b68a3ad2765b748855bc549ed895b7b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
